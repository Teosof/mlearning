# Лекция №7. Метод главных компонент (PCA)

Классический математически обоснованный метод. Пусть есть система координат в $n$-мерном пространстве
$$
\overrightarrow{e_1}, \overrightarrow{e_2}, ..., \overrightarrow{e_n} 
$$
 Эти координатные оси обладают свойствами:

1. Перпендикулярны друг другу.

2. $$
   \overrightarrow{e^T_i},\overrightarrow{e_j}=0,i \neq j
   $$

3. Каждая из осей имеет длину равную единице 
   $$
   \overrightarrow{e^T_i}, \overrightarrow{e_i}=1
   $$

В этой системе координат у нас задано $N$ точек, каждая из них задана вектором. 
$$
\overrightarrow{x_1}, \overrightarrow{x_2}, ..., \overrightarrow{x_N} 
$$

Для этих векторов проведем следующие процедуры:

1. От центруем, то есть вычтем из них среднее по всем векторам, таким образом среднее стало равно нулю. Стрелка над нулем
   обозначает, что $n$ штук нулей — это вектор, который состоит из нулевых координат.

   $$
   \frac{1}{N}\sum^N_{i=1}\overrightarrow{x_i}=0
   $$

2. Отнормируем на длину, каждое $\overrightarrow{x_i}$ разделим на сумму квадратов. Таким образом длина каждого вектора
   стала равна единице. 
   $$
   \overrightarrow{x^T_i}, \overrightarrow{x_i}=1
   $$

**Вес оси** - дисперсия проекции на эту ось всех наших точек, то есть суммирование идёт по $j$ от единицы до $N$, $\overrightarrow{e_i}$ - одна и та же ось, для которой мы вычисляем вес. $\overrightarrow{x_j}$ перемещается по всем векторам.
$$
\varphi(\overrightarrow{e_i})=\sigma^2_i=\frac{1}{N}\sum^N_{j=1}(\overrightarrow{e^T_j}\overrightarrow{x_j})^2=0
$$
Среднее для проекции будет равно нулю, поэтому его вычитать из $\overrightarrow{e^T_j}\overrightarrow{x_j}$ не требуется. Таким образом, остается сумма квадратов проекций. 

Нас интересует на сколько эти веса неравномерно распределены или наоборот — равномерно по осям координат и мерой равномерности (неравномерности) будем считать энтропию, то есть суммы весов умноженные на логарифмы весов. Так как энтропия считалась для бинарных данных и связывалась информацией, которая передается по каналу, там был двоичный логарифм. Для определенности мы будем использовать двоичный логарифм также. 
$$
H{\{\overrightarrow{e_1}, \overrightarrow{e_2}, ..., \overrightarrow{e_n}\}}=-\sum^n_{i=1}\varphi(\overrightarrow{e_i})\times \log_2\varphi(\overrightarrow{e_i})
$$
Чем меньше энтропия, тем более неравномерно распределены веса координат. 

Решение доказывается математически. Минимуму энтропии соответствует система координат составленная из собственных векторов ковариационной матрицы.
$$
C\overrightarrow{e}=\lambda \overrightarrow{e}
$$
где $C$ - ковариационная матрица.

Если мы умножаем какой-то вектор на матрицу, то мы получаем другой вектор, в общем случае, который не совпадает с первым, не только по длине, но и по направлению.

Если речь идет о собственном векторе, то его умножение на матрицу эквивалентно умножению его на какой-то скаляр, то есть у этого вектора останется, то же направление, как у исходного, но изменится длина в зависимости от того, какой была $\lambda$. $\lambda$ - собственное число, а $\overrightarrow{e}$ - собственный вектор.

Пусть существует $n$ штук разных собственных векторов, каждому из которых соответствует собственное число. При условии, что все $n$ собственных векторов будут определены как $\overrightarrow{e_1}, \overrightarrow{e_2}, ..., \overrightarrow{e_n}$. Расположим их в порядке убывания собственных чисел $\lambda_1,\lambda_2,..., \lambda_n$. 
Дополнительно обратим внимание, что собственное число равно дисперсии проекций на соответствующую ось:
$$
\sigma^2_i=\frac{1}{N}\sum^N_{j=1}(\overrightarrow{e^T_i}\overrightarrow{x_j})^2
$$
Отбросим $k$ штук с наименьшими весами, которые имеют наименьшие собственные числа. Ошибка представления исходного множества точек (ошибка представления исходных данных) может быть записана в процентах
$$
\xi^2=100 \times \frac{\sum^{n-k}_{i=1}\lambda_i}{\sum^{n}_{i=1}\lambda_i}\%
$$
Сумма от $1$ до $(n-k)$ - остаток собственных чисел. 

## Пример №1

Рассмотрим ковариационную матрицу:
$$
C=\begin{bmatrix}
  1 & 1 \\
  1 & 1
\end{bmatrix}
$$
Имеет двумерное пространство $e_1$ и $e_2$. При неком наборе $X$ и коэффициенте корреляции между $x_1$ и $x_2$, который равен единице.

Уравнение собственного числа и вектора:
$$
C\overrightarrow{e}=\lambda\overrightarrow{e}
$$
Алгоритм решения:
1. Переносим содержащее неизвестное в левую часть
2. Выносим $\overrightarrow{e}$ 
3. Умножим $\lambda$ на $I$, где $I$ - единичная матрица
$$
(C-\lambda I)\overrightarrow{e}=0
$$
4. Раскроем эту скобку и получи систему линейных уравнений, относительно неизвестных $e_1$ и $e_2$ с неизвестным $\lambda$. Полученная система однородная. Для её решения определить должен быть равен нулю, то есть
$$
|C-\lambda I|=0
$$
   
$$
\det\begin{bmatrix}
  1-\lambda & 1 \\
  1 & 1-\lambda
\end{bmatrix}=(1-\lambda)^2-1=0
$$
Решаем квадратное уравнение:

$$
\lambda_1=2,\lambda_2=0
$$
Подставляем  $\lambda_1$ в систему уравнений:
$$
(C-\lambda_1 I)\overrightarrow{e}=0
$$

$$
\begin{bmatrix}
  -1 & 1 \\ 1 & -1
\end{bmatrix}
\times
\begin{bmatrix}
  e_1 \\ e_2
\end{bmatrix}=0
$$

$$
-e_1+e_2=0 \\ e_1-e_2=0
$$

В этой системе с двумя неизвестными решением будет:
$$
e_1=e_2=const
$$
Первый собственный вектор равен:
$$
\overrightarrow{e_1}=\begin{bmatrix}
  c \\ c
\end{bmatrix}
$$

$$
C\overrightarrow{e_1}=
\begin{bmatrix}
  c \\ c
\end{bmatrix}=
\begin{bmatrix}
  2c \\ 2c
\end{bmatrix}=
2\times\begin{bmatrix}
  c \\ c
\end{bmatrix}
$$

В данной задаче способ определения $C$ не имеет значения, но для определённости возьмем его таким, чтобы длина $e_1=1$. 

Разделим $C$ на длину $\overrightarrow{e_1}$, извлечем корени и поделим на длину каждый компонент. В результате получим:
$$
\overrightarrow{e_1}=\begin{bmatrix}
  c \\ c
\end{bmatrix}=c^2+c^2=2c^2
$$
$$
\overrightarrow{e_1}=\begin{bmatrix}
  \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
\end{bmatrix}
$$

Проверка на APL:

```apl
	(2 2⍴1)+.×,['']÷2 2*.5 1.414213562
1.414213562
      ÷2 2*.5
0.7071067812 0.7071067812
      2×÷2 2*.5
1.414213562 1.414213562
```

Подставим $\lambda_2$ и найдем собственный вектор аналогично первому. Решением будет аналогичная константа с противоположными знаками:
$$
\overrightarrow{e_2}=\begin{bmatrix}
  \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}}
\end{bmatrix}
$$

Для единичной корреляционной матрицы с полученными собственными числами
$$
C=\begin{bmatrix}
  1 & 1 \\
  1 & 1
\end{bmatrix} \\ \rho_{x_1,x_2}=1
$$

Рассмотрим как будут лежать точки $x_1,x_2$ для $\rho_{x_1,x_2}=1$

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection7/1.png)

Точки будут лежать под углом 45 градусов. После центрирования среднее значение будет равно нулю, следовательно, коэффициент корреляций будет равен:
$$
\frac{\frac{1}{N}\sum((x_1-0)\times(x_2-0))}{\sum x^2_1 \times \sum x^2_2}=\frac{\frac{1}{N}\sum(x_1 \times x_2)}{\sum x^2_1 \times \sum x^2_2}=1
$$

$$
\rho_{12}\frac{\frac{1}{N}\sum(x_1-\overline{x_1})(x_2-\overline{x_2})}{\sum(x_1-\overline{x_1})^2(x_2-\overline{x_2})^2}
$$

Опустим проекцию на $e_1$. Сначала она была под 45 градусов, но теперь она равна нулю. В результате $e_2$ перпендикулярен к $e_1$. Все проекции $e_2$ равны нулю. 

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection7/2.png)

## Пример №2

Корреляционная матрица диагональная, то есть коэффициенты $x_1$ и $x_2$ равны нулю. 
$$
C=\begin{bmatrix}
1 & 0 \\ 0 & 1
\end{bmatrix}
$$

$$
C\overrightarrow{e}=\lambda\overrightarrow{e}\rightarrow(C-\lambda I)\overrightarrow{e}=0 \rightarrow |C-\lambda I|=0
$$

$$
\det\begin{bmatrix}
  1-\lambda & 1 \\
  1 & 1-\lambda
\end{bmatrix}=(1-\lambda)^2-1=0
$$

Решением будет:
$$
\lambda_1=1 \\ \lambda_2=1
$$
Найдем собственные векторы:
$$
\begin{bmatrix}
0 & 0 \\ 0 & 0
\end{bmatrix}
\times
\begin{bmatrix}
e_1 \\ e_2
\end{bmatrix}=0
$$
Соответственно любые $e_1$ и $e_2$ буду удовлетворять:
$$
0\times e_1 + 0\times e_2=0 \\ 0\times e_1 + 0\times e_2=0
$$
Крайне важно, чтобы эти вектора были перпендикулярны между собой, то есть $\rho_{12}=0$.

При выполнении этого условия точки будут распределены внутри определенного круга. При подсчете значений в квадрантах, мы получим сумму $X_{1i}$ и $X_{2i}$. Поскольку $\overline{x_1}=\overline{x_2}=0$, то в первом и четвертом квадранте мы получим большие положительные числа, а во втором и четвертом — большое отрицательное число. Объясняется это знаками. 

## Пример с температурным полем

Рассмотрим реактор с десятью термопарами на выходе и полем (слева холоднее, справа горячее). Произведем в разные моменты времени измерения. При сохранении распределения поля, средняя температура может плавать. 

Для детального анализа спроектируем функцию на APL, которая будет возвращать три параметра:

1. $a_1$ - собственные числа
2. $b_1$ - матрица собственных векторов
3. $c_1$ - матрица проекций на собственные вектора исходных значений. В данном случае $x_1$.

```apl
	x1←⍳10
	x1←⊃(⊂x1)+⍳16
  (a1 b1 c1)←mds.(COVM SELFIC)x1
```

1. Рассмотрим первые десять чисел
   
    ```apl
    	a1
    212.5 0 0 0 0 0 0 0 0 0
    ```
    
    Так как матрица ковариационная, то собственные числа — это нецелочисленные значения. Одно их них сильно отличается от других. 
    
    Если мы отбросим девять собственных векторов и оставим один, который соответствует первому числу, то мы сохраним 100% информации. 

2. Рассмотрим первый собственный вектор

    ```apl
    	b1[;1]
     ̄0.316227766  ̄0.316227766  ̄0.316227766  ̄0.316227766  ̄0.316227766
     ̄0.316227766  ̄0.316227766  ̄0.316227766  ̄0.316227766  ̄0.316227766
    ```

    Первые компонентные собственного вектора в 10-мерном пространстве равны между собой. Заметим, что они отрицательные. Если значения по модулю, как веса, то в качестве проекции имеем взвешенное значение с этими весами, одинаковыми сумма термопары. Таким образом, на самом деле, имеем не 10-мерное пространство, а одномерное, где меняет только средний уровень. 

Рассмотрим пример, в котором у нас меняется не только средний уровень, но и распределение. У нас меняется несколько вещей:

1. Средняя температура
2. Распределение температур

Для этого случая посчитаем собственные числа и вектора. 

Во-первых, мы видим, первых два собственных числа отличны от нуля в отличие от остальных. Мы понижаем размерность пространства с 10-мерного до 2-мерного. 

```apl
	x2←⍳10
	x2←⊃(⊂x2)×∊2⍴⊂⍳8 x2[⍳8;]←⌽x2[⍳8;]
	(a2 b2 c2)←mds.(COVM SELFIC)x2 a2
2103.75 1588.125 2.479131558E ̄13 8.187234765E ̄14 6.036530033E ̄14 1.97029597E ̄14 1.239658888E ̄14  ̄3.340844541E ̄14  ̄5.933993521E ̄14  ̄1.555394639E ̄13
```

Во-вторых, рассмотрим собственные вектора, которые мы будем проектировать:

```apl
    b2[;⍳2]
0.4954336943        ¯0.316227766
0.3853373178        ¯0.316227766
0.2752409413        ¯0.316227766
0.1651445648        ¯0.316227766
0.05504818826       ¯0.316227766
¯0.05504818826      ¯0.316227766
¯0.1651445648       ¯0.316227766
¯0.2752409413       ¯0.316227766
¯0.3853373178       ¯0.316227766
¯0.4954336943       ¯0.316227766
```

Второй вектор при проектировании десяти термопары покажет аналог сретений температуры на выходе. Первый покажет, что первая компонента соответствует первое термопаре и имеет положительно значение, а последняя компонента имеет такое же по модулю, но отрицательное значение. Такая разность будет характеризовать перекос поля. 

Рассмотрим ситуацию в центре реактора. Сохраняется одинаковость весов и противоположность знаков, но происходит убывание по модулю. Это связано с тем, что в середине реактора мы имеем маленькие изменения, а по краям — большие в зависимости от направления поля. Пятая и шестая термопары будут иметь минимальный вес, а крайние — максимальные. Такое явление называется перекосом. Первое собственное число — это перекос, второе — это среднее значение поля. 
