#  Лекция №14. Многомерный вероятностный метод классификации

## Аппроксимация гауссовским распределением

Пусть существует заданное $N_c$ - число классов. Для каждого из них оценим условную плотность распределения для векторов $x$. При условии, что они принадлежат классу $\omega_i$. Общая запись многомерного гауссовского распределения:
$$
p(\overrightarrow{x}|\omega_i)=\frac{1}{2\pi^{\frac{n}{2}}|C_i|^{\frac{1}{2}}\exp{[-\frac{1}{2}(\overrightarrow{x}-\overrightarrow{m_i})^TC^{-1}_i(\overrightarrow{x}-\overrightarrow{m_i})]}}
$$
$i = 1, 2, ..., N_c$ - по числу классов

$\overrightarrow{m_i}=\frac{1}{n_i}\sum_{\overrightarrow{x} \in \omega_i}\overrightarrow{x}$ - математическое ожидание

$C_i=\frac{1}{n_i}\sum_{\overrightarrow{x} \in \omega_i}(\overrightarrow{x}-\overrightarrow{m_i})(\overrightarrow{x}-\overrightarrow{m_i})^T$ - ковариационная матрица

$C_i$ - определитель

Рассмотрим двумерный случай:
$$
\overrightarrow{x}=\begin{bmatrix}
  x_1 \\ x_2
\end{bmatrix},
\overrightarrow{m}=\begin{bmatrix}
  m_1 \\ m_2
\end{bmatrix},
$$

$$
\frac{1}{n}\sum^n_{i=1}(\overrightarrow{x}-\overrightarrow{m_i})(\overrightarrow{x}-\overrightarrow{m_i})^T
$$

$$
\begin{bmatrix}
   x_1-m_1 \\ x_2-m_2
\end{bmatrix}
\begin{bmatrix}
  x_1-m_1,x_2-m2
\end{bmatrix}
$$

$$
\begin{bmatrix}
	(x_1-m_1)^2(x_1-m_1)(x_2-m_2) \\ (x_2-m_2)(x_1-m_1)(x_2-m_2)^2
\end{bmatrix}
$$

Рассмотрим последовательность $x$ - $\overrightarrow{x_1},\overrightarrow{x_2},...\overrightarrow{x_n}$. Следовательно, добавим индекс $i$
$$
\begin{bmatrix}
   x_{1i}-m_1 \\ x_{2i}-m_2
\end{bmatrix}
\begin{bmatrix}
  x_{1i}-m_1,x_{2i}-m2
\end{bmatrix}
$$

$$
\begin{bmatrix}
	(x_{1i}-m_1)^2(x_{1i}-m_1)(x_{2i}-m_2) \\ (x_{2i}-m_2)(x_{1i}-m_1)(x_{2i}-m_2)^2
\end{bmatrix}
$$

$$
\frac{1}{N}\sum^N_{i=1}(x_{1i}-m_1)^2=\sigma^2_1
$$

$$
\frac{1}{N}\sum^N_{i=1}(x_{2i}-m_2)^2=\sigma_2
$$

$$
\frac{1}{n}\sum(x_{1i}-m_1)(x_{2i}-m_2)=cov_{12}
$$

$cov_{12}$ - коэффициент ковариация между первым и вторым признаками. Наша матрица после суммирования и деления на $n$ выглядит следующим образом
$$
\begin{bmatrix}
	\sigma^2_1 & cov_12 \\
	cov_21 & \sigma^2_2
\end{bmatrix}
$$
В многомерном случае (при $n>2$):
$$
\begin{bmatrix}
	\sigma^2_1 & ... & ... & cov_{ij} \\
	... & \sigma^2_2 & ... & ... \\
	... & ... & ... & ... \\
	cov_{ji} &... & ... & \sigma^2_n \\
\end{bmatrix}
$$
Ковариация не зависит от порядка признака, поэтому данная матрица симметрична. 

**Формула Байеса** для условной вероятности: наблюдается класс $\omega_i$ при условии, что существует значение вектора $x$ - произведение априорной вероятности класса $\omega_i$ на полную вероятность $X$ при условии $\omega_i$, деленное на нормировочный множитель, характерный для каждого класса. 
$$
p(\omega_i|\overrightarrow{x})=\frac{p(\omega_i)\times p(\overrightarrow{x}|\omega_i)}{\sum^N_{i=1}p(\omega_i)\times p(\overrightarrow{x}|\omega_i)}
$$
Оценка вероятности — это фиксация набор измерений. Грубо говоря, вероятность, которую мы не можем экспериментально получить, мы получаем при помощи формулы Байеса. 

Имея на руках три класса, мы можем посчитать по формуле значения для каждого из них. Для упрощения этой процедуры, мы выбираем максимум, переходим к решающим функциям, которые приведут нас к аналогичному решению, что и при полном распределении. 

Для начала прологарифмируем выражение:
$$
d_i(\overrightarrow{x})=\ln{[p(\omega_i)\times p(\overrightarrow{x}|\omega_i)]}=\ln[p(\omega_i)]+\ln[p(\overrightarrow{x}|\omega_i)]
$$

$$
d_i(\overrightarrow{x})=\ln{[p(\omega_i)]}-\frac{n}{2}\ln(2\pi)-\frac{1}{2}\ln|C_i|-\frac{1}{2}(\overrightarrow{x}-\overrightarrow{m_i})^TC^{-1}_i(\overrightarrow{x}-\overrightarrow{m_i})
$$

При сравнении этих вероятностей имеем одинаковый знаменатель, поэтому он не важен, максимум останется прежним. Прологарифмируем числитель.

В общем случае, ковариационная матрица своя для каждого класса. Решающая функция будет квадратичной, значит наши измерения для объектов будут входить во второй степени после перемножения этих матриц. 

Если необходимо один класс отличить от другого, то нам нужны $d_1(x),d_2(x)$ и их максимум. Границей является разность этих решающих функция, прировненная к нулю. 
$$
d_i(\overrightarrow{x})=\ln[p(\omega_i)]-\frac{1}{2}\ln|C_i|-\frac{1}{2}(\overrightarrow{x}-\overrightarrow{m_i})^TC^{-1}_i(\overrightarrow{x}-\overrightarrow{m_i})
$$

$$
d_i(\overrightarrow{x})-d_j(\overrightarrow{x})=0
$$

### Упрощение решающей функции при разных предположениях

Одно из упрощений — одинаковость ковариационной матрицы для всех классов. В двумерном случае графическое отображение — это срез на некой высоте $b’$ перевернутого параболоида. 

Если мы предположим, что уровень среза одинаковый, то при нахождении меньших (больших) главы сторон эллипса, можно утверждать, что для этого класса у нас менее (более) сильная корреляция. 

Если мы предположим, что все $C$ равны, то видим, что эллипсы отличаются только средним значением. В этом случае логарифм будет одинаковый для всех $i$. При нахождении максимума определить ковариационный матрицы отбрасывается. 
$$
d_i(\overrightarrow{x})=\ln[p(w_i)]+\overrightarrow{x^T}C^{-1}\overrightarrow{m_i}-\frac{1}{2}\overrightarrow{m^T_i}C^{-1}\overrightarrow{m_i}
$$
где $\overrightarrow{m^T_i}C^{-1}\overrightarrow{m_i}$ - не включает $x$, некоторая константа для каждого класса, а $\overrightarrow{x^T}C^{-1}\overrightarrow{m_i}$ включает $x$, когда мы умножим обратную ковариационную матрицу на математическое ожидание, то получим некий числовой вектор. Домножим его вектор-столбец на вектор-строку и получим сумму:
$$
\sum^N_{i=1}a_ix_i
$$
$N$ - размерность. 

Решающая функция линейно зависит от $x$. Если взять разность решающих функций двух классов и приравнять ее к нулю (граничному значению), то получим выражение, которое демонстрирует, что граница — линейно относительно $x$. Если имеем неравные ковариационные матрицы, то она становится квадратичной относительно $x$.

Если ковариационные матрицы равны, то она будет для двумерного случая прямой, для трехмерного плоскостью и так далее, но по сути своей она остается линейной функцией от всех $x$.
Следующий шаг по упрощению — предположение, что ковариационные матрицы одинаковы и диагональные, то есть ковариация $i$ и $j$ равны нулю. Решающая функция:
$$
C_i=\sigma^2I,\forall{i}
$$

$$
p(\omega_i)=\frac{1}{N_c},\forall{i}
$$

$$
d_i(\overrightarrow{x})=\overrightarrow{x^T}\overrightarrow{m_i}-\frac{1}{2}\overrightarrow{m^T_i}\overrightarrow{m_i}
$$

Такая запись означает, что входящее скалярное произведение на математическое ожидание $i$-го класса. Графически этот случай совпадает со случаем эталонов. 

Пример диагностирования акустических шумов

```apl
	m←c LearnGauss_N s
	ρm
2
	ρ¨m
4 4
```

- Правый аргумент — матрица спектров акустического шума
- Левый аргумент — вектор из значений нормального теплообмена и кризиса теплообмена

В результате обучения получили вектор размерности два. Размерность каждого элемента равна четырем, так как необходимо определить качения априорной вероятность, ковариационный матрицы, математического ожидания и обратную ковариационную матрицу. 

Произведем классификацию спектров, сравнивая по байесовскому многомерному правилу с физической классификацией. 

Для первого случая упрощения мы снова проводим обучение.

Для каждого класса после обучения мы имеем три параметра, так как у всех одинаковый̆ определитель.

## KNN - k ближайших соседей

Простой алгоритм, который индифферентен к размерности признаков. $k$ - параметр алгоритма. 

Пусть $k=9$, тогда находим ближайшего соседа к точке с неизвестной классификацией второго, …, девятого ближайшего соседа и проводим круг, в центре которого расположена неизвестная точка, с радиусом для $k$-ого ближайшего соседа. 

Неизвестная точка относится к классу с максимальным числом кругов. В любом пространстве будет верна формула
$$
R_{xy}=\sqrt{\sum^n_{i=1}(x_i-y_i)^2}
$$

### Алгоритм

1. Считаем расстояние от точки $x$ до всех $y$
2. Располагаем их в порядке возрастания
3. Находим $k$-ую 
4. Для $R_{xy}\leqslant R_K$ подсчитываем число элементов каждого класса
5. Определяем максимальное представительство

Алгоритм работает не только для задач классификации, но и для задач регрессии. Когда мы хотим в какой-то точке оценить $y$, то при $k=3$ мы ищем три ближайшие точки и применяем усреднение: медиана, взвешивание и им подобные. В методе KNN регрессии, как и в KNN классификации, не имеет значение размерность пространства. 

### Пример реализации на Python

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

path = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

headernames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']

dataset = pd.read_csv(path, names = headernames)
dataset.head()

X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 4].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 8)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
result = confusion_matrix(y_test, y_pred)
print(f"Confusion Matrix: {result}")
report = classification_report(y_test, y_pred)
print(f"Classification Report: {report}")
accuracy = accuracy_score(y_test,y_pred)
print(f "Accuracy: {accuracy}")
```



## Метод потенциальных функций

Задача рассматривается в рамках разделения на два класса

### Алгоритм

1. К каждой точке одного класса приписываем первый положительный заряд
2. К каждой точке второго класса приписываем первый отрицательный заряд
3. Для неизвестной точки считаем потенциал. Если он положительный, то носим к первому классу. Если он отрицательный, то — ко второму классу. 

```apl
	disp x
	cl
1 1 2 2
	0 ap207.plot (⊂[1]⊃(cl=1/x)(⊂[1]⊃(cl=2)/x)
```

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection14/1.png)

Решающая функция при $\alpha=1$ представлена в виде:
$$
d(\overrightarrow{x})=e^{-\alpha\times[x^2_1+x^2_2]}-e^{-\alpha\times[(x_1-1)^2+(x_2-1)^2]}-e^{-\alpha\times[(x_1-1)^2+(x_2+1)^2]}+e^{-\alpha\times[(x_1-2)^2]+x^2_2]}
$$
