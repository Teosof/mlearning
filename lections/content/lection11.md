# Лекция №11. Случайный поиск с адаптацией

Пусть $dp$ лежит в области $(0;1)$ с поправкой вероятности выбора признака. 

Рассмотрим алгоритм. Сперва задается равномерное распределение вероятности выбора признака. В данном случае вероятность для каждого признака $0,01$.  Пусть $n$ - число выбираемых признаков (например, пять).  $r$ - число испытаний (например, десять).

Для определения $n$ каждый раз будем проверять оптимальный набор признаков на качество классификации. При условии, что оно является оптимальным, будем выбирать минимально возможное число признаков. Таки образом, мы десять раз выбираем случайный набор из ста объёмом пять. Для каждого из $r$ наборов посчитаем критерий качества. Например, для каждой пятерки признаков будем искать оптимальное направление Фишера, проецировать на него и считать какой одномерный критерий Фишера нам подходит. Набор, для которого оптимальное направление показывает наибольший одномерный критерий Фишера — лучший. Из $r$ выбираем наилучший и увеличиваем вероятность выбора в последующем $p\to p+dp$ этого признака. Для наихудшего набора уменьшаем вероятность последующего выбора худших признаков $p\to p-dp$.  

Таким образом проводим $R$ групп испытаний. После случайно выбранного испытаний выбираем наилучшую подсистему из $n$ признаков. Она часто оказывается в конце. 

Пусть $dp=0,01$, тогда признак, которые попадает в наихудшую группу, имеет нулевую вероятность появления. Такая результативность является негативной, так как из десяти выбранных признаков, девять могут отказаться отвратительным, а последний окажется положительным. По ошибке этот признак будет уничтожен. 

Если $dp=0$, то никакой адаптации не будет. В результате работы алгоритмы никак не изменятся вероятности выбора признаков, поэтому алгоритм сводится к случайному поиску. 

Пусть существует некая обучающая выборка. Матрица $X$ с $n$ признаками и вектор $Y$, у которого $y \in [1,2]$. Таким образом, будем рассматривать классификацию на два класса. 

Нужно выбрать $k<n$ наиболее информативных признаков для классификации. Самый простой и надёжный метод — это перебор всех возможных признаков, например:

```apl
	3!100
120
```

Для выбора лучшей тройки необходим критерий. Например, проекция многомерного Фишечное и вычисления для этих проекций одномерного критерия Фишера:
$$
F=\frac{(m_1-m_2)^2}{S_1+S_2}
$$
При росте $F$ растет полезность данной подсистемы. Предположим, что из десяти получились $i(the\_best)=3,6,9$ признаки, спроектируем их на плоскость. В полученном решении нет необходимости использовать случайный поиск и адаптацию. 

Допустим, что в задаче есть кризис теплообмена по спектрам акустического сигнала, то есть установлен ПЭП и с его помощью измеряется акустический сигнал. Мы можем вычислить спектральную плотность мощности (СПМ). Совершенно нормальным является оценивание СПМ на 200 частотах в диапазоне нахождения сигнала (ссылка на матрицу $X$). 

В подобных задачах всегда необходимо выбирать из имеющихся признаков информативные. Таким образом, мы решаем две задачи, когда выбираем подмножество:

1. Облегчение вычислений. Легче обратить матрицу $10 \times 10$, чем $200 \times 200$
2. Убрать мешающие признаки, которые нам не позволяют создать правильную классификацию.

Найдем десять наилучших признаков из 200:

```apl
	10!200
2.245100431Е16
```

Результат — крайне большое число. Для такой ситуации воспользуемся случайным поиском с адаптацией. Теперь перебрать нужно не $2\times 10^{16}$, а $r\times R$ испытаний, где $r$ - число испытаний в группе, $R$ - число групп испытаний. 

Для начала рассмотрим равномерное распределение вероятности каждого признака. В таком случае, $\frac{1}{n}$ - вероятность выбрать каждый из признаков. Для проведения $r$ испытаний по $k$ признакам $(k<n)$ в наилучшую группу перемещаем значения попавшиеся по $p \to p+dp$, в наихудшую группу перемещаем значения — по $p \to p-dp$. 

Наилучшая или наихудшая группа образуется по выбранному критерию качества, например, многомерный критерий Фишера:

1. Повторяем $r$ испытаний с новым распределением вероятность выбора для каждого признака. 
2. Перемещаем значения в наилучшую и наихудшую группу соответственно. 
3. Повторяем первый и второй пункт $R$ раз. 
4. Выбираем из $r\times R$ систему признаков. 

## Параметры алгоритма:

1. $k$ - количество необходимых признаков (чем меньше, тем лучше. Два — оптимальный вариант)

   Методика исследований в DataScience:

   1. Эксперимент с данным;
   2. Эксперимент с алгоритмами;

   3. Эксперимент с параметрами алгоритмов.

   Для решения каждой из задач в анализе данных существует множество методов. Такое разнообразие объясняется характеристикой случайно взятой задачи. Рассмотрим задачу, в которой необходимо отличать крестики от кружочков. При переборе всех возможных методов, задача будет решена разными путями. Задача сводится до анализа данных на однородность. Любой из алгоритмов кластерного анализа разобьет входные данные на два кластера. Полученные данные сильно различаются в зависимости от встающих перед нами задач. Поэтому требуется выбрать необходимый метод классификации. Ситуация, при которой кластерный анализ не работает с неким набором данных, невозможна. 

2. $dp$ - определение наилучшей или наихудшей группы. 

   $dp, \max=\frac{1}{n}$. Начальное распределение с вероятностью выбора для каждого признака $\frac{1}{n}$. Значения, которые превосходят $\frac{1}{n}$ взять в группы невозможно, так как отрицательных вероятностей не существует, поэтому $\max$ сводится к $\frac{1}{n}$.

   При использовании такого подхода мы можем случайно уничтожить хороший признак. 

   $dp, \min=0$. В данном случае нет никакой адаптации, только случайный поиск. Мы не отнимаем и не прибавляем ноль, то есть не имеем никакой полезности в информации. Адаптация исчезает. 

3. $r$ - число испытаний в группе

   $r, \min=2$. Например, группу попали оба плохих значений. Несправедливо плохое значение переместилось в наилучшую группу, хорошее значение - в наихудшую. Таким образом, выбор хороших и плохих значений всего лишь по двум признакам имеет высокую вероятность ошибки

   $r, \max=no!$. При росте максимального значений, увеличивается число вычислений. 

   $r, optimal=20$. Данные рассуждения подтверждаются, так как при $r=10$, вероятность, что они будут равномерно распределены стремится к единице. 

4. $R$ - число групп испытаний

   При заданном параметре происходит адаптация. Чем больше $R$, тем лучше. Мы стремимся к наилучшей системе из $k$ признаков. Следовательно, при проходе по $r*R$ выбираем наилучшую систему из $k$ признаков. 
   
   ```apl
   	best<-k SSA x1 x2 ⍝ x1 и x2 – матрицы для классов ω1 и ω2
   ```
   
   Рассмотрим ситуацию, при которой $k=5,n=100$. Наилучшая система $11, 93, 5, 44, 59$. Мы выбираем из ста признаков значения случайно, наша наилучшая группа не должна быть упорядоченной. Такой набор признаков обеспечит $90\%$ точность. 
   
   Применяя данный метод, мы сохраняем наилучший набор признаков. После $R$ шагов выполнения алгоритма мы получим распределение вероятности выбора признаков $p$. 
   
   Для улучшения результатов необходимо весь процесс начинать с точки старта. При добавлении в алгоритм уже полученного наилучшего распределения признаков, мы будем выбирать значения из этого распределения. Поправляя полученную выборку, мы продолжим процесс, а не начнём его заново. 