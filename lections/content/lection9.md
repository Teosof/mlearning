# Лекция №9. Критерий информативности признаков для распознания

Пусть существую признак $X_i$, где $i$ - столбец нашей матрицы $X$. Рассмотрим вектор $Y$. Возьмем все значения $X$, которые принадлежат первому классу и построим гистограмму ($\omega_1$ - класс). Аналогичным образом поступаем для $\omega_2$.

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection9/1.png)

Подобными рассуждения мы можем воспользоваться и для признака $X_j$.

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection9/2.png)

Признак $X_j$ графически лучше, чем $X_i$. Для описания этого математически возьмем модуль разности $m_1$ и $m_2$. Таким образом, мы оцениваем полезность разностью условных средних. 

От номера признака построим график:

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection9/3.png)

Самые информативные признаки обозначены красными точками. Рассмотрим распределение $(m_1-m_2)^2$. Большое число признаков находится около небольшого значения квадрата разности. Порог будет расположен в области, где значения больше, чем у остальных относительно квадрата разности средних. 

$k$ принадлежит множеству информативных признаков, при условии $(m^k_1-m^k_2)^2$ больше значения порога. При любом остатки этих признаков, в функцию $f$ попадет неполный вектор $X$. $X_k$ будет принадлежать множеству информативных признаков. 

В данный момент мы можем использовать любой метод, например, метод эталонов. Добавим набор информативных признаков, и посмотрим к какому центру наши значения оказались ближе. 

Дополнительно добавим еще два других признака и построим гистограммы, как это делали ранее. 

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection9/4.png)

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection9/5.png)

По нашим критериям признак $j$ лучше, но визуально лучше признак $i$. Проблема заключена в **изменчивости внутри классов**. Для $j$-го разность средних большая, но внутренние значения класса почти не меняются, следовательно, эти распределения сильно пересекаются. Таким образом, будет допущена ошибка: точка второго класса будет отнесена к первому или наоборот. 

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection9/6.png)

Признак $X_i$ имеет небольшую относительно $j$ разность средних по классам, но внутри каждого класса они меняются незначительно, следовательно, можно разделить два класса неким порогом:

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection9/7.png)

Кроме мер положения необходимо использовать меры изменчивости:

1. Среднеквадратичное отклонение
2. Сумма квадратов

$$
F=\frac{(m_1-m_2)^2}{S^2_1+S^2_2} 
$$

$$
m_1=\frac{1}{n_1}\sum_{x \in \omega_1}x=\frac{1}{n_1}\sum^{k}_{i=1}x \\
m_2=\frac{1}{n_2}\sum_{x \in \omega_2}x=\frac{1}{n_2}\sum^{m}_{i=k+1}x 
$$

$$
S^2_1=\sum_{x\in \omega_1}(x-m_1)^2 \\ S^2_2=\sum_{x\in \omega_2}(x-m_2)^2 \\
\sigma^2=\frac{1}{n}\sum_{x\in \omega}(x-m)^2
$$

Отличие $S^2_1$ от дисперсии отклонений по первому классу заключается в том, что дисперсия дополнительно делится на число точек в первом классе. Мы не делим значение на число точек, так как используем эту сумму и рассматриваем другую изменчивость. Если представителей одного класса больше, чем другого, то при делении на число точек они будут мало отличаться. Сумма $S$ отображает общую изменчивость в классах

## $F$ - критерий Фишера

По критерию Фишера $X_i$ будет лучше $X_j$, так как большое расстояние между средними поделится на такую же сумму квадратов отклонений по $\omega_1$ и $\omega_2$. Данный метод показывает себя хорошо при наличии “чистых” данных — отсутствие ошибочных значений и выборов. 

Рассмотрим ситуацию $F_j>F_i$, при которой в выборке существуют ошибочные значения или выбросы. 

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection9/8.png)

Расположение $m_1$ перемесится в сторону выбросов. Тоже произойдет и с $m_2$. Таким образом, разность средних окажется меньше, чем для признака $X_i$. Однако, если убрать выбросы, то $X_i$ будет хуже $X_j$.

Устранить выброс — использовать для вычисления мер положений робастные (устойчивые к выбросам и ошибкам в данных) методы оценивания мер положения. 

Заметим, что медиана может игнорировать выбросы. Следовательно, в робастном критерии Фишера мы вычислим $|med_1-med_2|$. Изменчивость, которая также чувствительна к выбросам, настраивается с помощью порога. Вместо суммы квадратов отклонений от среднего в знаменатели указывается межквартильный размах для $\omega_1$ и $\omega_2$.
$$
F_r=\frac{|med_1-med_2|}{R^{(1)}_{0.25}-R^{(2)}_{0.25}}
$$
В дополнении к робастному критерию Фишера рекомендуется использовать и стандартный критерий. В идеале, если мы имеем “чистую” выборку, то наши значения будут расположены на прямой, так как они являются более информативными признаками. 

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection9/9.png)

Если случайно выбранная точка по робастному критерию мало информативна, то значит, существуют отклонения в выборки по этому признаку. Таким образом, мы диагностируем проблемы в выборке. Если несколько точек ведут себя сходим образом, то мы выбираем множество информативных точек и строим функцию $f$.

