# Лекция №10. Многомерный критерий Фишера

Данный подход совпадает с линейным дискриминантным анализом. 

Рассмотри двухмерное пространство, где распределены два класса. Признак $X_1$ плох для классификации распознания этих классов, потому что проекции на тот признак из областей двух классов смешиваются, то же самое происходит и с признаком $X_2$. 

Найдем целевое направление проекции, на которое максимизирует одномерный̆ критерий Фишера и отделим один класс от другого. Построим вектор $f$ под углом $135^{\circ}C$. Построим проекции классов на это направление. Они не пересекаются. 

Разработаем алгоритм, в котором с шаговым $1^{\circ}C$ эта ось будет вращаться. На каждом градусе определим проекции, которые помогут определить критерий $F$. Сформируем отдельный массив градусов, которые соответствует максимальному критерию $F$. Для линейного дискриминантного анализа и для многомерного критерия Фишера существует простая математика:
$$
f=\frac{(m_1-m_2)^2}{S^2_1+S^2_2}
$$

$$
m_i=\frac{1}{n_i}\sum_{\overrightarrow{x} \in \omega_i}x,i=1,2,...,n
$$

$$
S_i = \sum_{x \in \omega_i}(x-m_i)^2
$$

Оптимальное направление $\omega$ для проектирования векторов $x$ в многомерном пространстве признаков определяется как:
$$
\overrightarrow{\omega}=S^{-1}_\omega(\overrightarrow{m_1}-\overrightarrow{m_2})
$$

$$
S_\omega=S_1+S_2
$$

$$
S_i=\sum_{\overrightarrow{x}\in \omega_i}(\overrightarrow{x}-\overrightarrow{m_i})(\overrightarrow{x}-\overrightarrow{m_i})^t
$$

где $i=1,2,…,n$ - ковариационные матрицы (с точностью до множителя $\frac{1}{n_i}$) для классов $\omega_1$ и $\omega_2$.

Среднее векторы для классов $\omega_1$ и $\omega_2$:
$$
\overrightarrow{m_i}=\frac{1}{n_i}\sum_{\overrightarrow{x}\in \omega_i}\overrightarrow{x}
$$
Искомые проекции векторов $x$ на оптимальное направление $\omega$ вычисляются как скалярное произведение:
$$
y=\overrightarrow{\omega^t}\overrightarrow{x}
$$
Реализация на APL:

```apl
	∇ W←Y fisher X;M1;M2 
[1] X←X-[2]M1←+⌿X÷≢X 
[2] Y←Y-[2]M2←+⌿Y÷≢Y 
[3] X←(⍉X)+.×X
[4] Y←(⍉Y)+.×Y
[5] W←(M1-M2)+.×⌹X+Y
[6] ∇
```

В программной реализации не имеет значения к какому конкретно классу привязана матрица. 

1 и 2 строки — вычисляются средние векторы по каждому классу и происходит центрирование данных.

3 и 4 строки — для первого и второго классов вычисляются ковариационные матрицы ($S_1$ и $S_2$).

5 строка — обращение суммы ковариационных матриц перемножается на разность векторов средних, получаем $W$ – направление.

Тестовые данные (матрица размером $20\times 2$ для первого и второго классов из значений от $0$ до $1$):

```apl
	x←?20 2⍴0
	y←  ̄3 3+[2]?20 2⍴0 
	plot ⊂[1] ̈x y
```

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/1.png)

Вычислим оптимальное направление. Спроектируем гистограмму с девятью интервалами (одну на другой):

```apl
	+w←x fisher y
 ̄1.065144169 0.9928572908
	9 hist2 (w+.×⍉x)(w+.×⍉y)
```

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/2.png)

Два класса прекрасно отличаются друг от друга. Для того чтобы провести классификацию, выберем порог. Если проекций меньше трех, то это будет $\omega_1$. Если больше, то - $\omega_2$.

Проиллюстрируем процесс в двумерном пространстве:

```apl
⍝ making projection
      proj←{⍺×(⍵+.×⍺)÷⍺+.×⍺} ⍝ функция для иллюстрации
      w proj y[1;]
 ̄3.044973696 2.838324073
 
⍝ apply with rank 1
      w proj⍤1⊢y[⍳2;]
¯3.044973696 2.838324073
¯2.832351409 2.640131571
 
⍝ prepare illustration on projections
      plot ⊂[1]¨x y ⍝ рисуем наборы точек
      color 'blue'
      marker w proj⍤1⊢y ⍝ проецируем их на оптимальную ось W для второго класса и проецируем их на оптимальную ось W для первого класса
      color 'black'
      marker w proj⍤1⊢x
      color 'gray'
      draw 0 0,[.5]w×5
```

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/3.png)

## Примеры

```apl
	a←(2/⍳10),[1.5]20⍴1 2 
	b←(2/⍳10),[1.5]20⍴3 4 0 
	plot ⊂[1] ̈a b
```

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/4.png)

Найдем оптимальное направление Фишера, которое будет максимизировать одномерный критерий Фишера. Данные заполнены в матрицах $a$ и $b$. Длина вектора (параллельного оси ординат) нам не нужна. Спроектируем оба класса на этот вектор и получим гистограмму. Порог будет равен $0,5$. 

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/5.png)

Если мы будем вращать эти данные по разным матрицам, то график преобразует. Критерий Фишера построит оси проекций с направлением право-низ ($135^{\circ}C$). Порог будет равен $0,5$.

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/6.png)

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/7.png)

Дополнительно повернем график и получим другое распределение. Фишер строит оси проекций с направлением вправо-вверх ($45^{\circ}C$). Порог будет равен $0,5$.

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/8.png)

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/9.png)

Рассмотри поведение многомерные примеров в шумовой диагностике:

```apl
	n←1 1 2 3 4 5 4 3 2 1 1 2 3 2 1
1
	c←1 1 2 3 4 5 4 3 2 1 1 1 1 1 1
1
	⍴c 16
	plotn (⍳16) n c
```

$n$ - для нормального распределения, $c$ - для аномального распределения. 

Отобразим как выглядят средние спектры:

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/10.png)

При низких частотах при норме и кризисе спектры совпадают. На высоких частотах при норме у нас отсутствует составляющая спектра, а при кризисе (аномалии) в районе 13 появляется пик (шум). 
$$
\sum\omega_iS_i=0+\omega_{12}S_{12}+\omega_{13}S_{13}+\omega_{14}S_{14}
$$

$$
|\sum^{11}_{i=1}\omega_iS_i+\sum^{14}_{i=12}\omega_iS_i| >>0 \\
\sum^{11}_{i=1}\omega_iS_i \to 0
$$

Если бы мы отличали значения только по двум точка, то когда проводим больше двух измерений, картина могла бы существенно измениться. 
$$
\left\{\begin{matrix}
x<p\Rightarrow\omega_1 \\ x>p\Rightarrow\omega_2
\end{matrix}\right.
$$
Всегда должна быть обучающая выборка большего объема (чем больше, тем лучше):

```apl
	a←20 16⍴n
	b←23 16⍴c 
	a1←a+.1× ̄2+?(⍴a)⍴3 
	b1←b+.1× ̄2+?(⍴b)⍴3 
	w←a1 Fisher b1 plot w
```

### Пример из курсовой работы

Рассматривалась диагностика кризиса теплообмена по характеристикам акустического шума, которые поступают с ПЭ датчика. Размерность пространства $200$  Спектр измерялся на $200$ разных частотах. Грубо сократим число признаков до десяти, но разрешение спектра ухудшится по частоте, но некритично. 

Посчитаем компоненты весов для оптимальных проекций Фишера. Затем спроектируем нормальные и кризисные спектры на это направление в десяти мерном пространстве:

```apl
    ⍴a←+/19 10 20⍴a
19 10
    ⍴b←+/21 10 20⍴b
21 10
    w←a Fisher b
    w
0.8478593433 1.377880251 0.7465740197 1.006157679 0.9072175876 ¯1.219360386
0.4514475061 1.120955995 1.471503214 0.63226001
    31 hist2 (w+.×⍉a)(w+.×⍉b)
```

Справа будет отображена норма, а слева кризис теплообмена. Пороговое значение равно $45$, что позволит с большим запасом различать наши классы. 

![График](/Users/roman/PycharmProjects/machine_learning/lections/images/lection10/11.png)

